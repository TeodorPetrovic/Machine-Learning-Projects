{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BAG OF WORDS PRIMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
      "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
     ]
    }
   ],
   "source": [
    "vocab = {}  # maps word to integer representing it | Mapiracemo svaku rec u jedan integer vrednost\n",
    "word_encoding = 1\n",
    "def bag_of_words(text):\n",
    "  global word_encoding\n",
    "\n",
    "  words = text.lower().split(\" \")  # Pravi listi svih reci u recenici, lowercase i deli reci na osnovu \" \" praznog polja(ZANAMERUJEMO GRAMATICKE GRESKE)\n",
    "  bag = {}  # stores all of the encodings and their frequency | Cuva zapis vrednosti svih reci i njiovu frekvenciju\n",
    "\n",
    "  for word in words:\n",
    "    if word in vocab:\n",
    "      encoding = vocab[word]  # get encoding from vocab | pribavljamo iz dictionary-a mapiranu vrednost ove reci(ta vrednost jeste integer)\n",
    "    else:\n",
    "      vocab[word] = word_encoding # Ako je nema u dict onda dodaj tu rec i dodeli joj word encoding vrednost koja je na pocetku 1 a posle kroz petlju se povecava\n",
    "      encoding = word_encoding #Localnu promenljuvu encoding postavi da je ista kao prethodna vrednost word_encoding\n",
    "      word_encoding += 1 #povecaj word encoding za 1\n",
    "    \n",
    "    if encoding in bag:\n",
    "      bag[encoding] += 1 # Ako postoju vec jedan instanca ove reci sa ovim encoding brojem povecaj njenu frekvenciju za 1\n",
    "    else:\n",
    "      bag[encoding] = 1 # Ako ne postoji stavi da joj je frekvencija 1\n",
    "  \n",
    "  return bag\n",
    "\n",
    "text = \"this is a test to see if this test will work is is test a a\"\n",
    "bag = bag_of_words(text)\n",
    "print(bag)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ovo se sve reci u listi ['this', 'is', 'a', 'test', 'to', 'see', 'if', 'this', 'test', 'will', 'work', 'is', 'is', 'test', 'a', 'a']\n",
      "Ovo je encoding u else-u 1\n",
      "Ovo je encoding u else-u 2\n",
      "Ovo je encoding u else-u 3\n",
      "Ovo je encoding u else-u 4\n",
      "Ovo je encoding u else-u 5\n",
      "Ovo je encoding u else-u 6\n",
      "Ovo je encoding u else-u 7\n",
      "Ovo je encoding u if-u 1\n",
      "Ovo je encoding u if-u 4\n",
      "Ovo je encoding u else-u 8\n",
      "Ovo je encoding u else-u 9\n",
      "Ovo je encoding u if-u 2\n",
      "Ovo je encoding u if-u 2\n",
      "Ovo je encoding u if-u 4\n",
      "Ovo je encoding u if-u 3\n",
      "Ovo je encoding u if-u 3\n",
      "{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
      "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
     ]
    }
   ],
   "source": [
    "text = \"this is a test to see if this test will work is is test a a\"\n",
    "words = text.lower().split(\" \")  # create a list of all of the words in the text, well assume there is no grammar in our text for this example\n",
    "print(\"Ovo se sve reci u listi\", words)\n",
    "\n",
    "vocab = {}  # maps word to integer representing it\n",
    "word_encoding = 1\n",
    "\n",
    "bag = {}  # stores all of the encodings and their frequency\n",
    "\n",
    "for word in words:\n",
    "    if word in vocab:\n",
    "      encoding = vocab[word]  # get encoding from vocab\n",
    "      print(\"Ovo je encoding u if-u\", encoding)\n",
    "    else:\n",
    "      vocab[word] = word_encoding\n",
    "      encoding = word_encoding\n",
    "      print(\"Ovo je encoding u else-u\", encoding)\n",
    "      word_encoding += 1\n",
    "    \n",
    "    if encoding in bag:\n",
    "      bag[encoding] += 1\n",
    "    else:\n",
    "      bag[encoding] = 1\n",
    "bagW = bag\n",
    "\n",
    "print(bagW)\n",
    "print(vocab)\n",
    "#Tamo se nalazi ona global vrednost sto znaci da prilkom svake pokretanje funkcije ona hvata globalnu vrednost i od nje pocinje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis - da li je recenica sa dobrim znacenjem ili losim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Movie Review Datase - sadrzi 20000 hiljada reviewa za filmove i organizovano je tako da sto je manji\n",
    "#broj to znaci da je taj rec ona sa najvecom frekvencijom u tekstu a najveci broj znaci da se rec jedva \n",
    "#pojavljuje u nasem tekstu\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.preprocessing\n",
    "\n",
    "VOCAB_SIZE = 88584\n",
    "MAXLEN = 250\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "(train_data, train_label), (test_data, test_label) = imdb.load_data(num_words=VOCAB_SIZE) #Ucitavanje podataka i odredjivanje test i trening skupova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prikaz nekog review-a u nasem trening skupu\n",
    "len(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S obzirom na to da nasi review-s nisu iste duzine to ce nam predstavljati problem prilikom treniranja naseg modela\n",
    "#Tj u nas trening model slaki review mora da bude iste velicine tj duzine. To cemo postici preko PADDING funkcije to\n",
    "#sledecim parametrima \n",
    "\n",
    "# 1. Ako review ima vise od 250 reci trimuj ostatak\n",
    "# 2. Ako review ima manje od 250 reci dodajemo ostatak tako da njegova duzina bude 250(ostaci ce imati vrednost 0 tako da nece uticati na nas trenig)\n",
    "\n",
    "#Nasrecu za nas KERAS vec ima funkciju koja ce resiti ovaj problem umesto da mi to radimo\n",
    "\n",
    "#Cisto da se napovene ova funkcija dodaje 0 sa leve strane teksta(tj brojeva u ovom slucaju jer smo sve reci konvertovali u brojeve)\n",
    "train_data = sequence.pad_sequences(train_data, MAXLEN) \n",
    "test_data = sequence.pad_sequences(test_data, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     1,   194,\n",
       "        1153,   194,  8255,    78,   228,     5,     6,  1463,  4369,\n",
       "        5012,   134,    26,     4,   715,     8,   118,  1634,    14,\n",
       "         394,    20,    13,   119,   954,   189,   102,     5,   207,\n",
       "         110,  3103,    21,    14,    69,   188,     8,    30,    23,\n",
       "           7,     4,   249,   126,    93,     4,   114,     9,  2300,\n",
       "        1523,     5,   647,     4,   116,     9,    35,  8163,     4,\n",
       "         229,     9,   340,  1322,     4,   118,     9,     4,   130,\n",
       "        4901,    19,     4,  1002,     5,    89,    29,   952,    46,\n",
       "          37,     4,   455,     9,    45,    43,    38,  1543,  1905,\n",
       "         398,     4,  1649,    26,  6853,     5,   163,    11,  3215,\n",
       "       10156,     4,  1153,     9,   194,   775,     7,  8255, 11596,\n",
       "         349,  2637,   148,   605, 15358,  8003,    15,   123,   125,\n",
       "          68, 23141,  6853,    15,   349,   165,  4362,    98,     5,\n",
       "           4,   228,     9,    43, 36893,  1157,    15,   299,   120,\n",
       "           5,   120,   174,    11,   220,   175,   136,    50,     9,\n",
       "        4373,   228,  8255,     5, 25249,   656,   245,  2350,     5,\n",
       "           4,  9837,   131,   152,   491,    18, 46151,    32,  7464,\n",
       "        1212,    14,     9,     6,   371,    78,    22,   625,    64,\n",
       "        1382,     9,     8,   168,   145,    23,     4,  1690,    15,\n",
       "          16,     4,  1355,     5,    28,     6,    52,   154,   462,\n",
       "          33,    89,    78,   285,    16,   145,    95])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kreiranje Naseg modela\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          2834688   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                8320      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,843,041\n",
      "Trainable params: 2,843,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 35s 55ms/step - loss: 0.3592 - acc: 0.8513 - val_loss: 0.3165 - val_acc: 0.8790\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 34s 54ms/step - loss: 0.2272 - acc: 0.9139 - val_loss: 0.2996 - val_acc: 0.8890\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 0.1753 - acc: 0.9359 - val_loss: 0.2954 - val_acc: 0.8886\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 38s 61ms/step - loss: 0.1450 - acc: 0.9474 - val_loss: 0.2869 - val_acc: 0.8998\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 34s 55ms/step - loss: 0.1277 - acc: 0.9556 - val_loss: 0.2885 - val_acc: 0.8994\n"
     ]
    }
   ],
   "source": [
    "#Treniranje naseg modela\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_data, train_label, epochs=5, validation_split=0.2) #koristi 0.2 dela training skupa za validaciju modela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 14s 18ms/step - loss: 0.3541 - acc: 0.8676 0s - loss: 0.3548 - acc\n",
      "[0.3541089594364166, 0.8676000237464905]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_label)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that', 'movie', 'was', 'just', 'amazing', 'so', 'amazing']\n",
      "[12, 17, 13, 40, 477, 35, 477]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
     ]
    }
   ],
   "source": [
    "#Istrenirali smo model sada na redu je da napravimo predikciju na osnovu neko reviewa za film\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "# print(word_index)\n",
    "\n",
    "def encode_textM(text):\n",
    "    tokens = keras.preprocessing.text.text_to_word_sequence(text) # uzmi recenicu i pretvori u listu reci\n",
    "    print(tokens)\n",
    "    tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "    print(tokens)\n",
    "    return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
    "\n",
    "text = \"that movie was just amazing, so amazing\"\n",
    "encoded = encode_textM(text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that movie was just amazing so amazing\n"
     ]
    }
   ],
   "source": [
    "#Ovo je decoder ovoga iz integger u tekst\n",
    "\n",
    "#Ovde je malo iskomplikovano ali samo je uradion smenu u dictionary {'Bela': 1} sad je {1 : 'Bela'}\n",
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "#print(reverse_word_index)\n",
    "\n",
    "def decode_integers(integers):\n",
    "    PAD = 0\n",
    "    text = \"\"\n",
    "    for num in integers:\n",
    "        if num != PAD:  #Ako integer nije 0 u text promenljivu upisi promenljuvu cije indeks se mapira u rec preko reverse_word indeks i dodaj prazno mesto\n",
    "            text += reverse_word_index[num] + \" \"\n",
    "    return text[:-1] #uzmi sve vrednosti promenljive text, samo nemoj uzeti poslednju koja u ovom slucaju je \" \"\n",
    "\n",
    "print(decode_integers(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'movie', 'was', 'so', 'awsome', 'i', 'really', 'loved', 'it', 'and', 'would', 'watch', 'it', 'again', 'because', 'it', 'was', 'amazingly', 'great']\n",
      "[11, 17, 13, 35, 23012, 10, 63, 444, 9, 2, 59, 103, 9, 171, 85, 9, 13, 2786, 84]\n",
      "[[0.8202393]]\n",
      "['that', 'movie', 'sucked', 'i', 'hated', 'it', 'and', \"wouldn't\", 'watch', 'it', 'again', 'was', 'one', 'of', 'the', 'worst', 'things', \"i've\", 'ever', 'watched']\n",
      "[12, 17, 2064, 10, 1797, 9, 2, 583, 103, 9, 171, 13, 28, 4, 1, 246, 180, 204, 123, 293]\n",
      "[[0.2743745]]\n"
     ]
    }
   ],
   "source": [
    "#Sada idemo na predikciju da radimo\n",
    "\n",
    "def predict(text):\n",
    "    encode_text = encode_textM(text)\n",
    "    pred = np.zeros((1, 250))\n",
    "    pred[0] = encode_text\n",
    "    result = model.predict(pred)\n",
    "    print(result)\n",
    "    \n",
    "positive_review = \"This movie was so awsome! I really loved it and would watch it again because it was amazingly great\"\n",
    "predict(positive_review)\n",
    "\n",
    "negative_review = \"that movie sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
    "predict(negative_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN PLAY GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1122304/1115394 [==============================] - 0s 0us/step\n",
      "1130496/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#Importovali smo Shakespere \n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'files' from 'collab' (C:\\Users\\Teodor Petrovic\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\collab\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\TEODOR~1\\AppData\\Local\\Temp/ipykernel_80700/3284830151.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcollab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpath_to_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'files' from 'collab' (C:\\Users\\Teodor Petrovic\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\collab\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from collab import files\n",
    "path_to_file = list(files.upload().keys())[0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "018610bf0173d81837d7f119329085b6b95de8057993cbd74162a07af92a39d0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
